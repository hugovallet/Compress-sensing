{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed sensing project : study of non-negative matrix factorization and online dictionary learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of recommender systems:\n",
    "### 1. Content-based approach\n",
    "They make profiles of users by identifying characteristic features about them: city, age, sex, etc. They also make profiles of items: type of product, product description, etc. However, there are 2 main limitations to this approach: on the one hand, it is a laborious task to collect all this information; on the other hand, collecting this information is a privacy-related problem.\n",
    "### 2. Collaborative filtering\n",
    "These systems only use features of past activities of the users: transaction history (such as Amazon does), user satisfaction expressed in ratings. Collaborative filtering algorithms lie in the following hypothesis: a group of people who have similar tastes in the past might also agree on their tastes in the future. Therefore,these algorithms seek to identify relationships between items and users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the early 1990s, several collaborative filtering-related papers were published to tackle the problem. The Netflix Prize, held from 2007 to 2009, boosted the interested in these algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, most of these algorithms rely on 2 mains parts: Matrix Factorization (MF) and Neighbor-based Approaches. Our project is mainly focused on MF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of Collaborative Filetring can be expressed as follows. We have:\n",
    "- $U$: set of $N$ users\n",
    "- $I$: set of $M$ items\n",
    "- $R$: ratings of items by users\n",
    "Therefore, $R$ is a matrix in which users are represented as rows and items are represented as columns. The idea of MF is to approximate by the product of 2 matrices $P$ and $Q$:\n",
    "$$ R \\approx P Q $$\n",
    "with $P$ is of size $N$ x $K$ and $Q$ is of size $K$ x $M$\n",
    "\n",
    "$P$ represents the features matrix of the users and $Q$ represents the features matrix of the items. Usually $K \\ll N,M$ which decreases the number of parameters needed to learn a model from $NM$ to $NK + KM$. The process of prediction comes as follows:\n",
    "1. We train a matrix factorization to obtain the 2 matrices $P$ and $Q$\n",
    "2. To predict a rating $ \\hat{r_{ui}}$ for a given item $i$ for a user $u$, we just take the $u^{th}$ row of $P$ and apply the dot product with the $i^{th}$ of $Q$:\n",
    "$$ \\hat{r_{ui}} = \\sum \\limits_{k=1}^{K} p_{uk} q_{ki} = \\mathbf{p_{u}} \\mathbf{q_{i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method mainly applies for batch methods: we assume that we have all the set of users to make our predictions of new $\\hat{r_{ui}}$. However, we will see in part IV that we can't apply the same reasoning for online methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind this statement is related to what is called $\\it Dictionary$  $\\it Learning$: let us consider a set $X = (x_1, . . . , x_N) ∈ \\mathbb{R}^{M×N}$ of $N$ signals of dimension $M$. Dictionary learning is a matrix factorization problem that aims to represent these signals as linear combinations of dictionary elements, denoted here by the columns of a matrix $D = (d_1, . . . , d_K) ∈ \\mathbb{R}^{M×K}$. More precisely, the dictionary $D$ is learned along with a matrix of decomposition coefficients $A = (\\alpha_1, . . . , \\alpha_n) \\in\n",
    "\\mathbb{R}^{K×N}$, so that $x_i \\approx D \\alpha_i $ for every signal $x_i$ and for a given loss metrics. For example, PCA can be seen as a $\\it Dictionary$ $\\it Learning$ task, in which the components of the dictionary are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the performance of our models with a validation set $V$, 2 main metrics are usued:\n",
    "- the Root Mean-Square Error:  $RMSE = \\sqrt{\\frac{1}{|V|} \\sum (\\hat{r_{ui}} - r_{ui})^2}$\n",
    "- the Absolute Mean Error: $AME = \\{\\frac{1}{|V|} \\sum |\\hat{r_{ui}} - r_{ui}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this project, we will only focuse on the RMSE, as it is the most common metrics used for Collaborative Filtering problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our $\\it Dictionary$ $\\it Learning$ project in 2 different tasks:\n",
    "1. Analyzing a batch method for collaborative filtering: Non Negative Matrix Factorization (NMF)\n",
    "2. Analyzing an online method: Online Structured Sparse Dictionary Learning (OSDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jester database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our 2 task, we will use the Jester dataset, which consists of 100 jokes rated by 24,983 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.82</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-8.98</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.08</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.00</td>\n",
       "      <td>8.35</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>5.73</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3      4     5     6     7     8      9      10   ...    \\\n",
       "0  -7.82   8.79  -9.66  -8.16 -7.52 -8.50 -9.85  4.17  -8.98  -4.76  ...     \n",
       "1   4.08  -0.29   6.36   4.37 -2.38 -9.66 -0.73 -5.34   8.88   9.22  ...     \n",
       "2  99.00  99.00  99.00  99.00  9.03  9.27  9.03  9.27  99.00  99.00  ...     \n",
       "3  99.00   8.35  99.00  99.00  1.80  8.16 -2.82  6.21  99.00   1.84  ...     \n",
       "4   8.50   4.61  -4.17  -5.39  1.36  1.60  7.04  4.61  -0.44   5.73  ...     \n",
       "\n",
       "     91     92     93     94     95     96     97     98     99     100  \n",
       "0   2.82  99.00  99.00  99.00  99.00  99.00  -5.63  99.00  99.00  99.00  \n",
       "1   2.82  -4.95  -0.29   7.86  -0.19  -2.14   3.06   0.34  -4.32   1.07  \n",
       "2  99.00  99.00  99.00   9.08  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "3  99.00  99.00  99.00   0.53  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "4   5.19   5.58   4.27   5.19   5.73   1.55   3.11   6.55   1.80   1.60  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = pd.ExcelFile('jester-data-1.xls')\n",
    "xl.sheet_names\n",
    "df = xl.parse('jester-data-1-new', header = None)\n",
    "nb_jokes_user = df[0]\n",
    "del df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24983 users\n",
      "100 items\n"
     ]
    }
   ],
   "source": [
    "n_users = df.shape[0]\n",
    "n_items = df.shape[1]\n",
    "print str(n_users) + ' users'\n",
    "print str(n_items) + ' items'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values (non rated elements of the rating matrix) are denoted by the value 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.82,   8.79,  -9.66, ...,  99.  ,  99.  ,  99.  ],\n",
       "       [  4.08,  -0.29,   6.36, ...,   0.34,  -4.32,   1.07],\n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       ..., \n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       [  2.43,   2.67,  -3.98, ...,  99.  ,  99.  ,  99.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.array(df)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we seperated the ratings matrix into 3 matrices: $R_{train}$, $R_{val}$ and $R_{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Partition the ratings into train / validation set\n",
    "R_train = ratings[:10000]\n",
    "R_val = ratings[10000:20000]\n",
    "R_test = ratings[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part : non-negative matrix factorization\n",
    "\n",
    "We wanted here to study Seung & Lee's \"Algorithms for Non-negative Matrix Factorization\" paper. They propose two types of non-negative factorization (NMF) : one that minimizes a least-squares distance and one that minimizes the Kullback-Leibler divergence (in the case that our initialization matrix is a probability matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of NMF is that, because of the nature of the algorithms implemented to obtain nonnnegative matrices, the nonnegative basis vectors that are\n",
    "learned are sparse combinations to generate expressiveness in\n",
    "the reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Least square NMF__\n",
    "\n",
    "Here is the problem setting:\n",
    "\n",
    "Given a matrix $V$ living in $\\mathbb{R}^{n,m}$ we want to find matrices $W\\in\\mathbb{R}^{n,k}$ and $H\\in\\mathbb{R}^{k,m}$ solutions of the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\text{minimize }||V-WH||^2 \\text{ under constraints } W,H>0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm, similarly to a classical gradient descent, will start with random $W$ and $H$. Then we will apply the following update rules allowing us to converge to a local minima (the problem is non convex w.r.t. $W$,$H$).\n",
    "\n",
    "$$\n",
    "H \\leftarrow H\\odot(W^TV)\\oslash(W^TWH)\n",
    "$$\n",
    "\n",
    "$$\n",
    "W \\leftarrow W\\odot(VH^T)\\oslash(WHH^T)\n",
    "$$\n",
    "\n",
    "With $\\odot$ standing for element-wise product and $\\oslash$ standing for element-wise division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_negative_threshold(Z):\n",
    "    neg_coefs = Z<0\n",
    "    Z[neg_coefs] = 0\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ls_solver(V,W,H,t=100,verbose=False):\n",
    "    \n",
    "    def update_h(V,W,H):\n",
    "        factor1 = np.dot(W.T,V)\n",
    "        factor2 = np.dot(np.dot(W.T,W),H)\n",
    "        return H*factor1/factor2\n",
    "\n",
    "    def update_w(V,W,H):\n",
    "        factor1 = np.dot(V,H.T)\n",
    "        factor2 = np.dot(W,np.dot(H,H.T))\n",
    "        return W*factor1/factor2\n",
    "\n",
    "    def error(V,W,H):\n",
    "        return lin.norm(V-np.dot(W,H))\n",
    "    \n",
    "    \n",
    "    \n",
    "    err=[]\n",
    "    for i in range(t):\n",
    "        W_new = update_w(V,W,H)\n",
    "        W = W_new\n",
    "        H_new = update_h(V,W,H)\n",
    "        H = H_new\n",
    "        if verbose==True:\n",
    "            print error(V,W,H)\n",
    "        err.append(error(V,W,H))\n",
    "    \n",
    "    return W,H,err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to slightly modify $R_{train}$ so that all elements are non negative, and that missing values are \"far\" from observed values: add 20 to all elements and put missing values to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.82,   8.79,  -9.66, ...,  99.  ,  99.  ,  99.  ],\n",
       "       [  4.08,  -0.29,   6.36, ...,   0.34,  -4.32,   1.07],\n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       ..., \n",
       "       [ -9.71,  -9.71,   6.7 , ...,  -9.71,  -9.61,  -9.81],\n",
       "       [  3.74,   3.93,   3.4 , ...,   4.9 ,   4.66,   4.22],\n",
       "       [ 99.  ,   7.33,   4.81, ...,  99.  ,  99.  ,  99.  ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_train = R_train + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train[R_train == 119] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure our predictions on $R_{train}$, we extract a dense matrix, i.e. a submatrix of $R_{train}$ with no missing values inside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "p = R_train.shape[1]\n",
    "for i in range(p):\n",
    "    non_zero_line = R_train[:,i].nonzero()[0]\n",
    "    R_train = R_train[non_zero_line]\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    if len(R_train[:,i].nonzero()[0]) < 4000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3112L, 100L)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train = R_train[:,:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove 10 item ratings per user, make our predictions on the matrix and compare it to the dense matrix $R_{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_test = R_train.copy()\n",
    "nb_items_to_remove = 10\n",
    "for j, jtem in enumerate(R_train):\n",
    "    ## Take 1st row of R_train\n",
    "    x = jtem.copy()\n",
    "\n",
    "    #remove nb_items_to_remove values of x_O_test\n",
    "    index_pred = sorted(np.random.choice(len(x), nb_items_to_remove, replace = False))\n",
    "    x[index_pred] = 0\n",
    "\n",
    "    R_test[j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24.08,  19.71,   0.  , ...,  21.12,  20.78,  27.52],\n",
       "       [ 13.83,  16.46,  20.44, ...,  11.02,  18.06,  13.01],\n",
       "       [ 26.84,  23.16,  29.17, ...,  15.78,  10.19,   0.  ],\n",
       "       ..., \n",
       "       [ 27.04,  27.14,   0.  , ...,  19.32,  27.77,  23.3 ],\n",
       "       [ 21.8 ,  19.81,  17.33, ...,  23.98,  18.93,  19.27],\n",
       "       [ 23.74,  23.93,  23.4 , ...,  24.81,  24.47,  24.85]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24.08,  19.71,  26.36, ...,  21.12,  20.78,  27.52],\n",
       "       [ 13.83,  16.46,  20.44, ...,  11.02,  18.06,  13.01],\n",
       "       [ 26.84,  23.16,  29.17, ...,  15.78,  10.19,  17.91],\n",
       "       ..., \n",
       "       [ 27.04,  27.14,  28.69, ...,  19.32,  27.77,  23.3 ],\n",
       "       [ 21.8 ,  19.81,  17.33, ...,  23.98,  18.93,  19.27],\n",
       "       [ 23.74,  23.93,  23.4 , ...,  24.81,  24.47,  24.85]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least-square NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = R_test.copy()\n",
    "\n",
    "n,m = V.shape\n",
    "k = 70\n",
    "iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = non_negative_threshold(15+np.random.randn(k, m))\n",
    "W = non_negative_threshold(15+np.random.randn(n,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W,H,err = ls_solver(V,W,H,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x39672710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31571fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(err)),err)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Euclidian distance\")\n",
    "plt.title(\"Evolution of the distance between V and WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0869901861758402"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = non_negative_threshold(np.random.randn(k, m))\n",
    "W = non_negative_threshold(np.random.randn(n,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.786352258408073"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = np.ones((k, m))\n",
    "W = non_negative_threshold(np.random.randn(n,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8615501841236597"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "- Convergence is fast: it converges after 200 iterations.\n",
    "- The running time is also fast: 5.75 s\n",
    "- RMSE seems good as random predictions give a worse RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Kullback-Leibler NMF__\n",
    "\n",
    "Here the problem setting is similar, the only diference resides in the distance minimized:\n",
    "\n",
    "Given a matrix $V$ living in $\\mathbb{R}^{n,m}$ we want to find matrices $W\\in\\mathbb{R}^{n,k}$ and $H\\in\\mathbb{R}^{k,m}$ solutions of the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\text{minimize }KL(V,WH)\\text{ under constraints } W,H>0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, Lee and Seung propose a multiplicative update rule rewritten using a similar paper \"MULTIPLICATIVE UPDATE RULES FOR NONNEGATIVE MATRIX FACTORIZATION WITH CO-OCCURRENCE CONSTRAINTS\" (http://sig.umd.edu/publications/Tjoa_ICASSP2_201003.pdf)\n",
    "\n",
    "$$\n",
    "H \\leftarrow H\\odot \\frac{W^T\\frac{V}{WH}}{W^T\\mathbb{1}^n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W \\leftarrow W\\odot \\frac{\\frac{V}{WH}H^T}{\\mathbb{1}^mH^T}\n",
    "$$\n",
    "\n",
    "with $\\odot$ the element-wise matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kl_solver(V,W,H,t=100,verbose=False):\n",
    "    \n",
    "    def update_h(V,W,H):\n",
    "        n,m = V.shape\n",
    "        unit_vector = np.ones((n,m))\n",
    "        factor1 = np.dot(W.T,V/np.dot(W,H))\n",
    "        factor2 = np.dot(W.T,unit_vector)\n",
    "        return H*factor1/factor2\n",
    "\n",
    "    def update_w(V,W,H):\n",
    "        n,m = V.shape\n",
    "        unit_vector = np.ones((n,m))\n",
    "        factor1 = np.dot(V/np.dot(W,H),H.T)\n",
    "        factor2 = np.dot(unit_vector,H.T)\n",
    "        return W*factor1/factor2\n",
    "    \n",
    "\n",
    "    def error(V,W,H):\n",
    "        return lin.norm(V-np.dot(W,H))\n",
    "    \n",
    "    \n",
    "    err=[]\n",
    "    for i in range(t):\n",
    "        W_new = update_w(V,W,H)\n",
    "        W = W_new\n",
    "        H_new = update_h(V,W,H)\n",
    "        H = H_new\n",
    "        if verbose==True:\n",
    "            print error(V,W,H)\n",
    "        err.append(error(V,W,H))\n",
    "    \n",
    "    return W,H,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = non_negative_threshold(15+np.random.randn(k, m))\n",
    "W = non_negative_threshold(15+np.random.randn(n, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "W,H,err = kl_solver(V,W,H,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x3975ac88>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDSFuugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x39cffac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(err)),err)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Euclidian distance\")\n",
    "plt.title(\"Evolution of the distance between V and WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.7058888277608251"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.167427340794898"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(W, ord ='fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "- Convergence is very fast: it converges after only 100 iterations.\n",
    "- The running time is slower: 9.3 s\n",
    "- RMSE is not very good compared to the first solver. The 2nd algorithm seems to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "### Advantages\n",
    "- Convergence for both algorithms is fast, and the Kullback-Leibler Solver is almost twice as fast as the Least-Square Solver.\n",
    "- The running time for both algorithms is quite fast (ODG: )\n",
    "\n",
    "### Drawbacks\n",
    "- Compared to other results with the Jester dataset, we should have obtained a RMSE score ranked between 7 and 4. To prove this assumption, we noticed that, given 2 random matrices $W$ and $H$, the RMSE was lower than for our 2 NMF algorithms.\n",
    "- This is due to overfitting. The matrix factorization can overfit for users with few (no more than $K$) ratings: assuming that the feature vectors $H$ of the items  rated by the user are linearly independent and $H$ does not change, there exists a user feature vector $W_u$ with zero training error. Thus, there is a potential for overfitting, if $\\eta$ and the extent of the change in $H$ are both small.\n",
    "- A common way to avoid overfitting is to apply regularization by fixing the dictionary $W$ and by penalizing the square of the Euclidean norm of the weights, i.e. the features matrix of items $H$. NMF induces sparsity a but, but it is not enough to avoid overfitting.\n",
    "- Another drawback is that these 2 algorithms cannot be used for real data, that is to say, they are not scalable. Indeed, the Jester dataset is very small compared to MovieLens for example. In addition, we have only taken a small part of this dataset.\n",
    "- Finally, the 2 algorithms don't deal with missing values: we can only measure the RMSE if the rating matrix is dense. However, this never appears with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Link with the paper K-Dimensional Coding Schemes in Hilbert Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to link the NMF to the paper, let's take a look at the reconstruction error defined in the paper:\n",
    "$$ f_T(x) = \\min_{y \\in Y}{\\| x - T y\\|^2} $$\n",
    "and\n",
    "$$ \\hat{y} = \\text{arg } \\min_{y \\in Y}{\\| x - T y\\|^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approximate $\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x)$, we use the empirical risk defined as follows: \n",
    "$$\\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i)$$\n",
    "which corresponds to the MSE in our case, with $ x_i$ a user i of the rating matrix $R$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NMF coding scheme, it is proved that the \"regret\" $ \\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i)$ is bounded, with at least probability $1 - \\delta$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i) \\leq \\frac{K}{\\sqrt{m}}\\left(14 \\| \\mathcal{T} \\|_{Y} + \\frac{b}{2} \\sqrt{\\ln{16 m \\| \\mathcal{T}}\\|_Y^2} \\right) + b \\sqrt{\\frac{\\ln{1/\\delta}}{2 m}}$$\n",
    "with the following notations:\n",
    "$$\\mathcal{T} = \\{T: T \\in \\mathcal{L}(\\mathbb{R}^K, H), \\|T e_k \\| = 1, \\langle T e_k , T e_l\\rangle \\geq 0, 1 \\leq k,l \\leq K \\}$$\n",
    "\n",
    "$$ \\| \\mathcal{T} \\|_Y = \\sup_{T \\in \\mathcal{T}}{\\| T \\|_Y} = \\sup_{T \\in \\mathcal{T}}{\\sup_{y \\in Y}{\\| T y\\|}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a proof of the convergence of our algorithms. Indeed it immediately implies uniform convergence in probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $T$ refers to the matrix $W$ - i.e. the dictionary $D$ we defined in the introduction. Let's check whether $W$ satisfy the properties required in the hypothesis:\n",
    " - Given that $W$ is a positive matrix, it satifies the conditions $ \\langle W e_k , W e_l\\rangle \\geq 0, 1 \\leq k,l \\leq 70 $.\n",
    " - $\\|W e_k \\| \\approx 1, 1 \\leq k \\leq 70$. Indeed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-Square Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = R_test.copy()\n",
    "n,m = V.shape\n",
    "k = 70\n",
    "iterations = 500\n",
    "H = non_negative_threshold(15+np.random.randn(k, m))\n",
    "W = non_negative_threshold(15+np.random.randn(n,k))\n",
    "W,H,err = ls_solver(V,W,H,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of column 0:  1.3985850952\n",
      "Norm of column 1:  0.998241035828\n",
      "Norm of column 2:  1.72506609504\n",
      "Norm of column 3:  1.58221194129\n",
      "Norm of column 4:  1.47005354001\n",
      "Norm of column 5:  1.32225609726\n",
      "Norm of column 6:  1.31890566993\n",
      "Norm of column 7:  1.70596567399\n",
      "Norm of column 8:  1.38241253814\n",
      "Norm of column 9:  1.23122887857\n",
      "Norm of column 10:  1.41073694138\n",
      "Norm of column 11:  1.20363722945\n",
      "Norm of column 12:  1.49338685499\n",
      "Norm of column 13:  1.1990835909\n",
      "Norm of column 14:  1.33311646003\n",
      "Norm of column 15:  1.58139065052\n",
      "Norm of column 16:  1.57766343388\n",
      "Norm of column 17:  1.41127822659\n",
      "Norm of column 18:  1.50142833847\n",
      "Norm of column 19:  1.3842879854\n",
      "Norm of column 20:  1.33100805053\n",
      "Norm of column 21:  1.11407641039\n",
      "Norm of column 22:  1.33265710946\n",
      "Norm of column 23:  1.41863387635\n",
      "Norm of column 24:  1.35505420255\n",
      "Norm of column 25:  1.36422207547\n",
      "Norm of column 26:  1.39493474621\n",
      "Norm of column 27:  1.41770862127\n",
      "Norm of column 28:  1.36457104831\n",
      "Norm of column 29:  1.38039514212\n",
      "Norm of column 30:  1.33499306177\n",
      "Norm of column 31:  1.22447552531\n",
      "Norm of column 32:  1.59273165895\n",
      "Norm of column 33:  1.40548374937\n",
      "Norm of column 34:  1.55531045848\n",
      "Norm of column 35:  1.25840004748\n",
      "Norm of column 36:  1.38325325886\n",
      "Norm of column 37:  1.35096723562\n",
      "Norm of column 38:  1.8063530614\n",
      "Norm of column 39:  1.57252704778\n",
      "Norm of column 40:  1.07430984497\n",
      "Norm of column 41:  1.2791089191\n",
      "Norm of column 42:  1.47703160766\n",
      "Norm of column 43:  1.30560418133\n",
      "Norm of column 44:  1.28968379772\n",
      "Norm of column 45:  1.51344510649\n",
      "Norm of column 46:  1.25387229265\n",
      "Norm of column 47:  1.45235549585\n",
      "Norm of column 48:  1.52908730718\n",
      "Norm of column 49:  1.29191425252\n",
      "Norm of column 50:  1.69606992389\n",
      "Norm of column 51:  1.47216233403\n",
      "Norm of column 52:  1.45163079163\n",
      "Norm of column 53:  1.32862402548\n",
      "Norm of column 54:  1.23895544495\n",
      "Norm of column 55:  1.43140917758\n",
      "Norm of column 56:  1.55359051247\n",
      "Norm of column 57:  1.30523390689\n",
      "Norm of column 58:  1.19834798942\n",
      "Norm of column 59:  1.56479887647\n",
      "Norm of column 60:  1.36275211678\n",
      "Norm of column 61:  1.51961529863\n",
      "Norm of column 62:  1.23808629949\n",
      "Norm of column 63:  1.3566194424\n",
      "Norm of column 64:  1.21838588322\n",
      "Norm of column 65:  1.47962553622\n",
      "Norm of column 66:  1.05937192705\n",
      "Norm of column 67:  1.09051711164\n",
      "Norm of column 68:  1.39593064456\n",
      "Norm of column 69:  1.39178725656\n"
     ]
    }
   ],
   "source": [
    "for j in range(W.shape[1]):\n",
    "    print (\"Norm of column %d: \" % j) ,  np.linalg.norm(W[:,j], ord = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Kullback-Leibler Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = R_test.copy()\n",
    "n,m = V.shape\n",
    "k = 70\n",
    "iterations = 500\n",
    "H = non_negative_threshold(15+np.random.randn(k, m))\n",
    "W = non_negative_threshold(15+np.random.randn(n,k))\n",
    "W,H,err = kl_solver(V,W,H,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of column 0:  1.47279310423\n",
      "Norm of column 1:  1.62778248731\n",
      "Norm of column 2:  1.14866280956\n",
      "Norm of column 3:  1.25904924263\n",
      "Norm of column 4:  1.35587655604\n",
      "Norm of column 5:  1.5104290751\n",
      "Norm of column 6:  1.49384253434\n",
      "Norm of column 7:  1.37784581972\n",
      "Norm of column 8:  1.51946618142\n",
      "Norm of column 9:  1.68194205381\n",
      "Norm of column 10:  1.29927824895\n",
      "Norm of column 11:  1.38778289358\n",
      "Norm of column 12:  1.38309419013\n",
      "Norm of column 13:  1.45861430178\n",
      "Norm of column 14:  1.38974469203\n",
      "Norm of column 15:  1.45997425027\n",
      "Norm of column 16:  1.46615159815\n",
      "Norm of column 17:  1.41331561201\n",
      "Norm of column 18:  1.15663516734\n",
      "Norm of column 19:  1.46024616625\n",
      "Norm of column 20:  1.53083620917\n",
      "Norm of column 21:  1.44149884773\n",
      "Norm of column 22:  1.58931060511\n",
      "Norm of column 23:  1.57523418427\n",
      "Norm of column 24:  1.62703320033\n",
      "Norm of column 25:  1.26825428218\n",
      "Norm of column 26:  1.46133530719\n",
      "Norm of column 27:  1.418140273\n",
      "Norm of column 28:  1.47069349832\n",
      "Norm of column 29:  1.48944055069\n",
      "Norm of column 30:  1.58120926347\n",
      "Norm of column 31:  1.69859726055\n",
      "Norm of column 32:  1.05930309115\n",
      "Norm of column 33:  1.21689315078\n",
      "Norm of column 34:  1.69344007865\n",
      "Norm of column 35:  1.53304518855\n",
      "Norm of column 36:  1.33239065598\n",
      "Norm of column 37:  1.45669139725\n",
      "Norm of column 38:  1.25371042753\n",
      "Norm of column 39:  1.37863460055\n",
      "Norm of column 40:  1.25719422108\n",
      "Norm of column 41:  1.49552427802\n",
      "Norm of column 42:  1.37471433971\n",
      "Norm of column 43:  1.32772203292\n",
      "Norm of column 44:  1.52011440812\n",
      "Norm of column 45:  1.43231707725\n",
      "Norm of column 46:  1.63172531457\n",
      "Norm of column 47:  1.01980310515\n",
      "Norm of column 48:  1.45923948048\n",
      "Norm of column 49:  1.71227863168\n",
      "Norm of column 50:  1.31632256824\n",
      "Norm of column 51:  1.71492466446\n",
      "Norm of column 52:  1.43674263247\n",
      "Norm of column 53:  1.23624018816\n",
      "Norm of column 54:  1.16042206372\n",
      "Norm of column 55:  1.40225476677\n",
      "Norm of column 56:  1.58856475418\n",
      "Norm of column 57:  1.29653202652\n",
      "Norm of column 58:  1.4396316241\n",
      "Norm of column 59:  1.38523918481\n",
      "Norm of column 60:  1.7585473744\n",
      "Norm of column 61:  1.26207652833\n",
      "Norm of column 62:  1.68444488392\n",
      "Norm of column 63:  1.53188797987\n",
      "Norm of column 64:  1.41333572742\n",
      "Norm of column 65:  1.43954073752\n",
      "Norm of column 66:  1.47476895745\n",
      "Norm of column 67:  1.1923266188\n",
      "Norm of column 68:  1.58813819876\n",
      "Norm of column 69:  1.34174915704\n"
     ]
    }
   ],
   "source": [
    "for j in range(W.shape[1]):\n",
    "    print (\"Norm of column %d: \" % j) ,  np.linalg.norm(W[:,j], ord = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\| \\mathcal{T} \\|_Y = \\sqrt{K} $ and $b = 1$ so in our case, the bound becomes:\n",
    "    $$\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i) \\leq \\frac{K}{\\sqrt{m}}\\left(14 \\sqrt{K} + \\sqrt{\\ln{16 m K}} \\right) + \\sqrt{\\frac{\\ln{1/\\delta}}{2 m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $K = 70$, $m = 3112$ and we take $\\delta = 0.05$ (the inequality is verified with probability $0.95$):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.842397802297"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B =(70 / np.sqrt(3112))*(14 * np.sqrt(70) + np.sqrt(np.log(15 * 3112 * 70)) + np.sqrt(np.log(1/0.95)/(2 * 3112)))\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means\n",
    "$$\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i) \\leq 151.84$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To clarify some points, this bound doesn't require that $T e_k$ be positive: it only requires that $\\langle T e_k, T e_l\\rangle \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Third part: Online Structured Dictionary Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: Put some bound from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_alpha = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24983L, 100L)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4983L, 100L)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation de l'entrée dans l'algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On initialise D, $\\alpha$, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, d_x = R_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-b9c6828c9097>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "np.zeros(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Initlialize of parameters \n",
    "#D = np.random.randn(d_x,d_alpha)\n",
    "D = np.ones((d_x, d_alpha))\n",
    "alpha = np.random.randn(d_alpha)\n",
    "T_alpha = 5\n",
    "epsilon = 10**(-5)\n",
    "eta = 0.5\n",
    "kappa = 1. / (2**10)\n",
    "#d_G = np.ones(d_alpha)\n",
    "d_G = group_structure(d_alpha, struct='lasso', nb_group=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_G[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.84747649e-01,   8.11172050e-01,  -3.35750806e-01,\n",
       "        -1.12404880e+00,  -2.11700797e+00,   1.53686972e-01,\n",
       "        -1.13571637e-01,   6.34650571e-04,  -1.57193346e+00,\n",
       "        -4.43974897e-01,   2.46123279e-02,  -6.30150665e-01,\n",
       "        -1.61351667e+00,   4.32873320e-02,  -1.18885647e+00,\n",
       "         1.22877871e-01,  -4.98188002e-02,   2.65509225e-01,\n",
       "         2.21436049e-01,   8.65187145e-01,   1.36105312e-02,\n",
       "         9.07371389e-01,   4.98420555e-01,  -3.09835006e-01,\n",
       "         8.61622053e-03,   1.04723217e-02,  -6.92108324e-01,\n",
       "        -2.08681447e-01,   2.66401056e+00,   1.25761839e+00,\n",
       "        -1.05331790e+00,   7.75537849e-01,  -5.58575504e-01,\n",
       "         3.81421518e-01,   6.12055754e-02,   8.46009397e-01,\n",
       "        -1.54450499e+00,   2.97788119e-01,   1.43402966e+00,\n",
       "        -5.42858343e-01,  -1.45433048e+00,  -1.32153065e+00,\n",
       "        -4.88246422e-01,  -3.86288907e-01,  -7.70457081e-02,\n",
       "         6.88895056e-02,  -7.59731946e-01,   6.17517613e-01,\n",
       "        -3.39936550e-01,   7.97592105e-01,  -1.16969386e+00,\n",
       "        -7.46184605e-01,   7.34473559e-01,  -6.66309356e-01,\n",
       "         4.05961103e-01,  -1.78323294e+00,  -8.38913804e-02,\n",
       "        -6.18664674e-01,   2.09438614e-01,   4.48911556e-01,\n",
       "        -7.54721723e-01,   3.92139265e-02,   1.59998505e-01,\n",
       "        -2.03229481e-01,  -1.56785597e+00,  -1.19786297e+00,\n",
       "        -5.62480272e-01,  -1.31571471e-01,   1.27834590e+00,\n",
       "        -9.99943482e-01,   2.01457609e-01,  -6.45782631e-01,\n",
       "         5.58926072e-01,   1.80262558e+00,   1.74923005e+00,\n",
       "        -5.50254510e-01,  -1.50439283e+00,   1.33070002e-01,\n",
       "         1.94964746e+00,   3.22838027e-01,  -6.40173489e-01,\n",
       "         6.29118137e-01,  -1.37719641e+00,   1.08159678e+00,\n",
       "        -2.71071227e+00,   9.11132541e-01,  -6.12988301e-02,\n",
       "        -1.27813232e+00,  -1.32135275e+00,  -7.49103930e-01,\n",
       "         4.60771333e-02,   1.61962788e-01,  -1.55369730e+00,\n",
       "        -5.72335747e-01,  -1.45365836e+00,   2.91531621e-01,\n",
       "        -8.47021621e-01,   1.39565912e-01,   6.08941854e-01,\n",
       "        -1.12366805e+00])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On extrait $x_O$ de $x$, $D_O$ de $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Take 1st row of R_train\n",
    "x = list(R_train[0].copy())\n",
    "\n",
    "## Save indexes of observed values\n",
    "indices = [i for i, item in enumerate(x) if item != 99]\n",
    "\n",
    "#remove missing values of first row of R_train\n",
    "x_O = [a for a in x if a != 99]\n",
    "\n",
    "# save the observed rows of D\n",
    "D_O = D[np.array(indices),:].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def min_alpha(x_O, D_O, alpha, eta, d_alpha, T_alpha, epsilon, d_G):\n",
    "    ## Minimisation on alpha\n",
    "    for t in range(T_alpha):\n",
    "\n",
    "        ## Compute z\n",
    "        norm_G_alpha = []\n",
    "\n",
    "        for j in d_G:\n",
    "            norm_G_alpha.append(np.linalg.norm(j * alpha, ord =2))\n",
    "        coef = (np.linalg.norm(norm_G_alpha, ord = eta) ** (eta - 1))\n",
    "\n",
    "        norm_G_alpha = np.array(norm_G_alpha)\n",
    "        norm_G_alpha = np.power(norm_G_alpha, 2 - eta)\n",
    "        z = norm_G_alpha * coef\n",
    "        for i, item in enumerate(z):\n",
    "            z[i] = max(item, epsilon)\n",
    "\n",
    "        \n",
    "        ## Compute alpha\n",
    "        ksi = []\n",
    "        for j in range(d_alpha):\n",
    "            coef_ksi = 0.0\n",
    "            for i, item in enumerate(z):\n",
    "                coef_ksi += (d_G[i][j])**2 / item \n",
    "            ksi.append(coef_ksi)\n",
    "        \n",
    "        ### Pour résoudre le pb de progammation quadratique, on utilise cvxopt\n",
    "        # P = kappa * diag(ksi) + D_O.T.dot(D_O)\n",
    "        # Q = - D_O.T.dot(x_O)\n",
    "        P = kappa * np.diag(ksi) + D_O.T.dot(D_O)\n",
    "        P = cvxopt.matrix(P)\n",
    "\n",
    "        q = - D_O.T.dot(x_O)\n",
    "        q = cvxopt.matrix(q)\n",
    "\n",
    "        sol = cvxopt.solvers.qp(P,q)\n",
    "        alpha = np.ravel(sol['x'])\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = min_alpha(x_O, D_O, alpha, eta, d_alpha, T_alpha, epsilon, d_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.42423503e-03,   7.73132758e-01,  -1.92842830e-03,\n",
       "         6.39557785e-01,   8.44078406e-01,   1.65145383e+00,\n",
       "        -2.02564547e+00,  -1.38509686e+00,   2.55241164e+00,\n",
       "         3.58248673e+00,   2.14158321e-01,   1.39804015e+00,\n",
       "         1.21190967e+00,   1.19172084e-02,  -7.39976438e-01,\n",
       "        -8.84294653e-01,  -2.61427167e+00,  -9.83050675e-01,\n",
       "         3.04163367e+00,  -8.99400128e-02,  -1.20852488e-03,\n",
       "        -2.16870925e-03,   3.13649581e+00,  -7.44198137e-01,\n",
       "        -2.06542475e-03,  -2.38728310e-01,   1.31282960e+00,\n",
       "        -6.23508621e-01,   8.72839144e-01,   1.22546233e+00,\n",
       "         6.13006627e-01,  -1.77600661e+00,  -3.69670619e-01,\n",
       "         1.35573352e-01,  -8.35067040e-03,   1.52061564e+00,\n",
       "         1.03232727e+00,  -5.54533526e-01,   1.93495981e+00,\n",
       "         1.61971959e-02,   2.57337339e+00,   3.58793363e-01,\n",
       "         9.93485401e-01,  -1.34915583e-02,   2.51974941e+00,\n",
       "        -1.50184326e+00,  -7.60379104e-01,  -6.34946053e-03,\n",
       "        -2.33953169e+00,   1.96795652e+00,  -3.73324806e-03,\n",
       "         9.95423825e-01,  -1.55634505e-03,   2.92112238e-01,\n",
       "         5.81061186e-04,  -1.23438750e+00,   1.34260324e-03,\n",
       "         3.61187942e+00,   5.83503443e-01,   2.55954511e+00,\n",
       "        -8.16081162e-01,  -2.78235743e+00,  -3.62693745e-01,\n",
       "         1.37973781e+00,   2.45601089e-03,  -9.03804311e-01,\n",
       "        -1.20649960e+00,   1.30891750e+00,  -1.80647121e+00,\n",
       "         2.52941218e+00,  -2.52573487e+00,  -3.44364501e+00,\n",
       "         2.31917885e+00,   7.90788519e-01,  -1.06430733e-03,\n",
       "         6.24151504e-03,   3.88599828e-02,   2.90992992e+00,\n",
       "         2.46618557e-02,   3.85173150e+00,   1.53671181e-01,\n",
       "        -1.84653565e-01,  -2.51416112e+00,   2.50556225e-03,\n",
       "        -3.46158117e-04,  -3.31410984e+00,  -8.25911544e-02,\n",
       "        -1.20379215e+00,   1.63655327e+00,  -1.15221676e+00,\n",
       "         3.84226720e+00,   5.99880020e-04,   1.74736220e-03,\n",
       "         6.31589391e-01,  -6.33251346e-04,   2.27607568e+00,\n",
       "         2.33009511e-03,  -1.21812935e+00,   1.61649442e+00,\n",
       "         2.12409211e+00])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testing_model(D, alpha, nb_items_to_remove, R_test, model = 'NMF', missing_value = 99):\n",
    "    ## Define testing set\n",
    "    # On définit le testing set, on fait la prédiction et on calcule l'erreur dans 1 même boucle\n",
    "    \n",
    "    N = R_test.shape[0]\n",
    "    MSE = 0.0\n",
    "    #if model == 'NMF':\n",
    "     #   XXXXXXXX\n",
    "    if model == 'OSDL':\n",
    "        for j, jtem in enumerate(R_test):\n",
    "            ## Take 1st row of R_train\n",
    "            x = jtem.copy()\n",
    "\n",
    "            ## Save indexes of observed values\n",
    "            indices = [i for i, item in enumerate(x) if item != missing_value]\n",
    "            indices = np.array(indices)\n",
    "\n",
    "            # save the observed rows of D\n",
    "            # we will use for prediction of x_O_pred and compare it to x_O\n",
    "            D_O_test = D[np.array(indices),:].copy() \n",
    "\n",
    "            #remove missing values of first row of R_train\n",
    "            x_O_test = x[indices]\n",
    "\n",
    "            #remove nb_items_to_remove values of x_O_test\n",
    "            index_pred = sorted(np.random.choice(len(x_O_test), len(x_O_test) - nb_items_to_remove, replace = False))\n",
    "            x_O_pred = x_O_test[index_pred].copy()\n",
    "            D_O_pred = D[index_pred,:].copy()\n",
    "\n",
    "            #prediction\n",
    "            alpha_test = min_alpha(x_O_pred, D_O_pred, alpha, eta, d_alpha, T_alpha, epsilon, d_G)\n",
    "\n",
    "            MSE += mean_squared_error(x_O_test, D_O_test.dot(alpha_test))\n",
    "\n",
    "        MSE = MSE / N\n",
    "        RMSE = np.sqrt(MSE)\n",
    "    return 'MSE: ',MSE, ' RMSE: ', RMSE\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MSE: ', 21.379577341578543, ' RMSE: ', 4.6238055042982227)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_model(D, alpha, 10, R_test, model = 'OSDL', missing_value = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def group_structure(d_alpha, struct, nb_group=3):\n",
    "    if struct == 'lasso':\n",
    "        d_G = []\n",
    "        for i in range(d_alpha):\n",
    "            G = np.zeros(d_alpha)\n",
    "            G[i] = 1\n",
    "            d_G.append(G)\n",
    "            \n",
    "    if struct == 'group_lasso':\n",
    "        # fonction rand pour donner le nombre d'indices dans chaque partition de {1...d_alpha}\n",
    "        # Exemple: n paritions\n",
    "        partition = sorted(np.random.choice(d_alpha, nb_group-1, replace = False))\n",
    "\n",
    "        ## shuffle les indices\n",
    "        shuffled_indices = np.random.choice(d_alpha, d_alpha, replace = False)\n",
    "        \n",
    "        ## Création des groupes\n",
    "        x = []\n",
    "        for i, item in enumerate(partition):\n",
    "            if i == 0:\n",
    "                x.append(shuffled_indices[:item])\n",
    "            elif i == len(partition) - 1:\n",
    "                x.append(shuffled_indices[partition[i-1]:item])\n",
    "                x.append(shuffled_indices[item:])\n",
    "            else:\n",
    "                x.append(shuffled_indices[partition[i-1]: item])\n",
    "        d_G = []\n",
    "        for i, item in enumerate(x):\n",
    "            G = np.zeros(d_alpha)\n",
    "            G[item] = 1\n",
    "            d_G.append(G)\n",
    "            \n",
    "    return d_G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_structure(d_alpha, struct='lasso', nb_group=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
