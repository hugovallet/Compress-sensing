{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed sensing project : study of non-negative matrix factorization and online dictionary learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students : Hugo Vallet, Guillaume Carbajal\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Summary :\n",
    "## I / Introduction\n",
    "## II / Some definitions\n",
    "## III / The Jester dataset\n",
    "## III / Implementation and testing of Seung & Lee paper \"Non-negative Matrix Factorization\"\n",
    "#### With the Euclidian distance as metric\n",
    "#### With the Kullback Leibler divergence as metric\n",
    "## IV / Study of the theorical bound for NMF proposed by Maurer & Pontil\n",
    "## V / Going further than NMF : group-structured sparse online matrix factorization\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I / Introduction\n",
    "\n",
    "There are 2 types of recommender systems:\n",
    "\n",
    "### 1. Content-based approach\n",
    "They make profiles of users by identifying characteristic features about them: city, age, sex, etc. They also make profiles of items: type of product, product description, etc. However, there are 2 main limitations to this approach: on the one hand, it is a laborious task to collect all this information; on the other hand, collecting this information is a privacy-related problem.\n",
    "\n",
    "### 2. Collaborative filtering\n",
    "These systems only use features of past activities of the users: transaction history (such as Amazon does), user satisfaction expressed in ratings. Collaborative filtering algorithms lie in the following hypothesis: a group of people who have similar tastes in the past might also agree on their tastes in the future. Therefore,these algorithms seek to identify relationships between items and users.\n",
    "\n",
    "Since the early 1990s, several collaborative filtering-related papers were published to tackle the problem. The Netflix Prize, held from 2007 to 2009, boosted the interested in these algorithms. \n",
    "\n",
    "Nowadays, most of these algorithms rely on 2 mains parts: Matrix Factorization (MF) and Neighbor-based Approaches. Our project is mainly focused on MF.\n",
    "\n",
    "# II / Some definitions\n",
    "The problem of Collaborative Filetring can be expressed as follows. We have:\n",
    "- $U$: set of $N$ users\n",
    "- $I$: set of $M$ items\n",
    "- $R$: ratings of items by users\n",
    "Therefore, $R$ is a matrix in which users are represented as rows and items are represented as columns. The idea of MF is to approximate by the product of 2 matrices $P$ and $Q$:\n",
    "$$ R \\approx P Q $$\n",
    "with $P$ is of size $N$ x $K$ and $Q$ is of size $K$ x $M$\n",
    "\n",
    "$P$ represents the features matrix of the users and $Q$ represents the features matrix of the items. Usually $K \\ll N,M$ which decreases the number of parameters needed to learn a model from $NM$ to $NK + KM$. The process of prediction comes as follows:\n",
    "1. We train a matrix factorization to obtain the 2 matrices $P$ and $Q$\n",
    "2. To predict a rating $ \\hat{r_{ui}}$ for a given item $i$ for a user $u$, we just take the $u^{th}$ row of $P$ and apply the dot product with the $i^{th}$ of $Q$:\n",
    "$$ \\hat{r_{ui}} = \\sum \\limits_{k=1}^{K} p_{uk} q_{ki} = \\mathbf{p_{u}} \\mathbf{q_{i}} $$\n",
    "\n",
    "This method mainly applies for batch methods: we assume that we have all the set of users to make our predictions of new $\\hat{r_{ui}}$. However, we will see in part IV that we can't apply the same reasoning for online methods.\n",
    "\n",
    "The intuition behind this statement is related to what is called $\\it Dictionary$  $\\it Learning$: let us consider a set $X = (x_1, . . . , x_N) ∈ \\mathbb{R}^{M×N}$ of $N$ signals of dimension $M$. Dictionary learning is a matrix factorization problem that aims to represent these signals as linear combinations of dictionary elements, denoted here by the columns of a matrix $D = (d_1, . . . , d_K) ∈ \\mathbb{R}^{M×K}$. More precisely, the dictionary $D$ is learned along with a matrix of decomposition coefficients $A = (\\alpha_1, . . . , \\alpha_n) \\in\n",
    "\\mathbb{R}^{K×N}$, so that $x_i \\approx D \\alpha_i $ for every signal $x_i$ and for a given loss metrics. For example, PCA can be seen as a $\\it Dictionary$ $\\it Learning$ task, in which the components of the dictionary are orthogonal.\n",
    "\n",
    "In order to measure the performance of our models with a validation set $V$, 2 main metrics are used:\n",
    "- the Root Mean-Square Error:  $RMSE = \\sqrt{\\frac{1}{|V|} \\sum (\\hat{r_{ui}} - r_{ui})^2}$\n",
    "- the Absolute Mean Error: $AME = \\{\\frac{1}{|V|} \\sum |\\hat{r_{ui}} - r_{ui}|$\n",
    "\n",
    "In this project, we will only focuse on the RMSE, as it is the metric used by Seung&Lee and by Scabo & Lorincz (the 2 main papers we studied).\n",
    "\n",
    "We define our $\\it Dictionary$ $\\it Learning$ project in 2 different tasks:\n",
    "1. Analyzing a batch method for collaborative filtering: Non Negative Matrix Factorization (NMF)\n",
    "2. Analyzing an online method: Online Structured Sparse Dictionary Learning (OSDL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III / The Jester dataset\n",
    "\n",
    "For our 2 task, we will use the Jester dataset, which consists of 100 jokes rated by 24,983 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.82</td>\n",
       "      <td>8.79</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-9.85</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-8.98</td>\n",
       "      <td>-4.76</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>-5.63</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.27</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>9.08</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.00</td>\n",
       "      <td>8.35</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.80</td>\n",
       "      <td>8.16</td>\n",
       "      <td>-2.82</td>\n",
       "      <td>6.21</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>...</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>99.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-4.17</td>\n",
       "      <td>-5.39</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.04</td>\n",
       "      <td>4.61</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>5.73</td>\n",
       "      <td>...</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.27</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.73</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.11</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3      4     5     6     7     8      9      10   ...    \\\n",
       "0  -7.82   8.79  -9.66  -8.16 -7.52 -8.50 -9.85  4.17  -8.98  -4.76  ...     \n",
       "1   4.08  -0.29   6.36   4.37 -2.38 -9.66 -0.73 -5.34   8.88   9.22  ...     \n",
       "2  99.00  99.00  99.00  99.00  9.03  9.27  9.03  9.27  99.00  99.00  ...     \n",
       "3  99.00   8.35  99.00  99.00  1.80  8.16 -2.82  6.21  99.00   1.84  ...     \n",
       "4   8.50   4.61  -4.17  -5.39  1.36  1.60  7.04  4.61  -0.44   5.73  ...     \n",
       "\n",
       "     91     92     93     94     95     96     97     98     99     100  \n",
       "0   2.82  99.00  99.00  99.00  99.00  99.00  -5.63  99.00  99.00  99.00  \n",
       "1   2.82  -4.95  -0.29   7.86  -0.19  -2.14   3.06   0.34  -4.32   1.07  \n",
       "2  99.00  99.00  99.00   9.08  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "3  99.00  99.00  99.00   0.53  99.00  99.00  99.00  99.00  99.00  99.00  \n",
       "4   5.19   5.58   4.27   5.19   5.73   1.55   3.11   6.55   1.80   1.60  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = pd.ExcelFile('jester-data-1.xls')\n",
    "xl.sheet_names\n",
    "df = xl.parse('jester-data-1-new', header = None)\n",
    "nb_jokes_user = df[0]\n",
    "del df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24983 users\n",
      "100 items\n"
     ]
    }
   ],
   "source": [
    "n_users = df.shape[0]\n",
    "n_items = df.shape[1]\n",
    "print str(n_users) + ' users'\n",
    "print str(n_items) + ' items'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values (non rated elements of the rating matrix) are denoted by the value 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.82,   8.79,  -9.66, ...,  99.  ,  99.  ,  99.  ],\n",
       "       [  4.08,  -0.29,   6.36, ...,   0.34,  -4.32,   1.07],\n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       ..., \n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       [ 99.  ,  99.  ,  99.  , ...,  99.  ,  99.  ,  99.  ],\n",
       "       [  2.43,   2.67,  -3.98, ...,  99.  ,  99.  ,  99.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.array(df)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we seperated the ratings matrix into 3 matrices: $R_{train}$, $R_{val}$ and $R_{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Partition the ratings into train / validation set\n",
    "R_train = ratings[:10000]\n",
    "R_val = ratings[10000:20000]\n",
    "R_test = ratings[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "## IV / Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "We wanted here to study Seung & Lee's \"Algorithms for Non-negative Matrix Factorization\" paper. They propose two types of non-negative factorization (NMF) : one that minimizes a least-squares distance and one that minimizes the Kullback-Leibler divergence (in the case that our initialization matrix is a probability matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square NMF\n",
    "\n",
    "Here is the problem setting:\n",
    "\n",
    "Given a matrix $V$ living in $\\mathbb{R}^{n,m}$ we want to find matrices $W\\in\\mathbb{R}^{n,k}$ and $H\\in\\mathbb{R}^{k,m}$ solutions of the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\text{minimize }||V-WH||_2^2\\text{ under constraints } W,H>0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm, similarly to a classical gradient descent, will start with random W and H. Then we will apply the following update rules allowing us to converge to a local minima (the problem is non convex w.r.t. W,H).\n",
    "\n",
    "$$\n",
    "H \\leftarrow H\\odot\\frac{W^TV}{W^TWH}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W \\leftarrow W\\odot\\frac{VH^T}{WHH^T}\n",
    "$$\n",
    "\n",
    "With $\\odot$ standing for element-wise product and $-$ standing for element-wise division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_negative_threshold(Z):\n",
    "    neg_coefs = Z<0\n",
    "    Z[neg_coefs] = 0\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ls_solver(V,W,H,t=100,verbose=False):\n",
    "    \n",
    "    def update_h(V,W,H):\n",
    "        factor1 = np.dot(W.T,V)\n",
    "        factor2 = np.dot(np.dot(W.T,W),H)\n",
    "        return H*factor1/factor2\n",
    "\n",
    "    def update_w(V,W,H):\n",
    "        factor1 = np.dot(V,H.T)\n",
    "        factor2 = np.dot(W,np.dot(H,H.T))\n",
    "        return W*factor1/factor2\n",
    "\n",
    "    def error(V,W,H):\n",
    "        return lin.norm(V-np.dot(W,H))\n",
    "    \n",
    "    \n",
    "    \n",
    "    err=[]\n",
    "    for i in range(t):\n",
    "        W_new = update_w(V,W,H)\n",
    "        W = W_new\n",
    "        H_new = update_h(V,W,H)\n",
    "        H = H_new\n",
    "        if verbose==True:\n",
    "            print error(V,W,H)\n",
    "        err.append(error(V,W,H))\n",
    "    \n",
    "    return W,H,err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to slightly modify Rtrain so that all elements are non negative, and that missing values are \"far\" from observed values: add 20 to all elements and put missing values to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_train\n",
    "R_train = R_train + 20\n",
    "R_train[R_train == 119] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure our predictions on Rtrain, we extract a dense matrix, i.e. a submatrix of Rtrain with no missing values inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "p = R_train.shape[1]\n",
    "for i in range(p):\n",
    "    non_zero_line = R_train[:,i].nonzero()[0]\n",
    "    R_train = R_train[non_zero_line]\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    if len(R_train[:,i].nonzero()[0]) < 4000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3112L, 100L)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train = R_train[:,:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we remove 10 item ratings per user, make our predictions on the matrix and compare it to the dense matrix Rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_test = R_train.copy()\n",
    "nb_items_to_remove = 10\n",
    "for j, jtem in enumerate(R_train):\n",
    "    ## Take 1st row of R_train\n",
    "    x = jtem.copy()\n",
    "\n",
    "    #remove nb_items_to_remove values of x_O_test\n",
    "    index_pred = sorted(np.random.choice(len(x), nb_items_to_remove, replace = False))\n",
    "    x[index_pred] = 0\n",
    "\n",
    "    R_test[j] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.  ,  19.71,  26.36, ...,  21.12,  20.78,  27.52],\n",
       "       [ 13.83,  16.46,  20.44, ...,  11.02,  18.06,   0.  ],\n",
       "       [ 26.84,   0.  ,  29.17, ...,  15.78,  10.19,  17.91],\n",
       "       ..., \n",
       "       [ 27.04,  27.14,  28.69, ...,   0.  ,  27.77,  23.3 ],\n",
       "       [ 21.8 ,  19.81,  17.33, ...,  23.98,  18.93,   0.  ],\n",
       "       [ 23.74,  23.93,  23.4 , ...,  24.81,  24.47,   0.  ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24.08,  19.71,  26.36, ...,  21.12,  20.78,  27.52],\n",
       "       [ 13.83,  16.46,  20.44, ...,  11.02,  18.06,  13.01],\n",
       "       [ 26.84,  23.16,  29.17, ...,  15.78,  10.19,  17.91],\n",
       "       ..., \n",
       "       [ 27.04,  27.14,  28.69, ...,  19.32,  27.77,  23.3 ],\n",
       "       [ 21.8 ,  19.81,  17.33, ...,  23.98,  18.93,  19.27],\n",
       "       [ 23.74,  23.93,  23.4 , ...,  24.81,  24.47,  24.85]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of convergence of the algorithm\n",
    "\n",
    "The algorithm, according to seung & Lee, is able to converge to a local minima. Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time ellapsed :  5.20599985123\n"
     ]
    }
   ],
   "source": [
    "V = R_test.copy()\n",
    "n,m = V.shape\n",
    "k = 70 #components\n",
    "iterations = 500\n",
    "H = non_negative_threshold(15+np.random.randn(k, m)) #Initialize matrices randomly, respecting the non negativity constraints\n",
    "W = non_negative_threshold(15+np.random.randn(n,k)) #Initialize matrices randomly, respecting the non negativity constraints\n",
    "start = time()\n",
    "W,H,err = ls_solver(V,W,H,iterations)\n",
    "end = time()\n",
    "print \"Total time ellapsed : \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2097d3c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSi9eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdc923c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(err)),err)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Euclidian distance\")\n",
    "plt.title(\"Evolution of the distance between V and WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE after the training of a Dictionary with euclidian distance :  8.74967277281\n",
      "RMSE with purely random dictionary :  12.0439296406\n"
     ]
    }
   ],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)\n",
    "print \"RMSE after the training of a Dictionary with euclidian distance : \", RMSE\n",
    "\n",
    "H2 = non_negative_threshold(np.random.randn(k, m))\n",
    "W2 = non_negative_threshold(np.random.randn(n,k))\n",
    "MSE = mean_squared_error(R_train, W2.dot(H2))\n",
    "RMSE = np.sqrt(MSE)\n",
    "RMSE\n",
    "print \"RMSE with purely random dictionary : \", RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- Convergence is fast: it converges after 200 iterations.\n",
    "- The running time is also fast: 5.75 s\n",
    "- RMSE seems good as random predictions give a worse RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler NMF\n",
    "\n",
    "Here the problem setting is similar, the only diference resides in the distance minimized:\n",
    "\n",
    "Given a matrix $V$ living in $\\mathbb{R}^{n,m}$ we want to find matrices $W\\in\\mathbb{R}^{n,k}$ and $H\\in\\mathbb{R}^{k,m}$ solutions of the following minimization problem:\n",
    "\n",
    "$$\n",
    "\\text{minimize }KL(V,WH)\\text{ under constraints } W,H>0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, Lee and Seung propose a multiplicative update rule\n",
    "\n",
    "$$\n",
    "H \\leftarrow H\\odot \\frac{W^T\\frac{V}{WH}}{W^T\\mathbb{1}^{n,m}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W \\leftarrow W\\odot \\frac{\\frac{V}{WH}H^T}{\\mathbb{1}^{n,m}H^T}\n",
    "$$\n",
    "\n",
    "with $\\odot$ the element-wise matrix product, $-$ the element-wise division and $\\mathbb{1}^{n,m}$ the real-valued matrix of size $(n,m)$ with all its coefficients equal to 1.\n",
    "\n",
    "_Note for the reader : the updates rules here are not exactly the same form as want we can read in Seung & Lee's paper. In fact, to avoid the for-loops to compute the summing terms in Seung & Lee's updates rules, one can use the following paper_ http://sig.umd.edu/publications/Tjoa_ICASSP2_201003.pdf _, formula (5), for the Kullback divergence based udpdate. In all cases, with the notations we used, it is trivial to see that the two formulations are strictly the same !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_solver(V,W,H,t=100,verbose=False):\n",
    "    \n",
    "    def update_h(V,W,H):\n",
    "        n,m = V.shape\n",
    "        unit_vector = np.ones((n,m))\n",
    "        factor1 = np.dot(W.T,V/np.dot(W,H))\n",
    "        factor2 = np.dot(W.T,unit_vector)\n",
    "        return H*factor1/factor2\n",
    "\n",
    "    def update_w(V,W,H):\n",
    "        n,m = V.shape\n",
    "        unit_vector = np.ones((n,m))\n",
    "        factor1 = np.dot(V/np.dot(W,H),H.T)\n",
    "        factor2 = np.dot(unit_vector,H.T)\n",
    "        return W*factor1/factor2\n",
    "    \n",
    "\n",
    "    def error(V,W,H):\n",
    "        return lin.norm(V-np.dot(W,H))\n",
    "    \n",
    "    \n",
    "    err=[]\n",
    "    for i in range(t):\n",
    "        W_new = update_w(V,W,H)\n",
    "        W = W_new\n",
    "        H_new = update_h(V,W,H)\n",
    "        H = H_new\n",
    "        if verbose==True:\n",
    "            print error(V,W,H)\n",
    "        err.append(error(V,W,H))\n",
    "    \n",
    "    return W,H,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time ellapsed :  9.00300002098\n"
     ]
    }
   ],
   "source": [
    "H = non_negative_threshold(15+np.random.randn(k, m)) #Same initialization as before\n",
    "W = non_negative_threshold(15+np.random.randn(n, k)) #Same initialization as before\n",
    "\n",
    "start = time()\n",
    "W,H,err = kl_solver(V,W,H,iterations)\n",
    "end = time()\n",
    "print \"Total time ellapsed : \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x26050d30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7M++/9rCydBIcm3kQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20987160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(err)),err)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"KL distance\")\n",
    "plt.title(\"Evolution of the distance between V and WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE after the training of a Dictionary with KL distance :  8.74967277281\n"
     ]
    }
   ],
   "source": [
    "MSE = mean_squared_error(R_train, W.dot(H))\n",
    "RMSE = np.sqrt(MSE)\n",
    "print \"RMSE after the training of a Dictionary with KL distance : \", RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- Convergence is very fast: it converges after only 100 iterations.\n",
    "- The running time is slower: 9.3 s\n",
    "- RMSE is not very good compared to the first solver. The 2nd algorithm seems to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "### Advantages\n",
    "- Convergence for both algorithms is fast, and the Kullback-Leibler Solver is almost twice as fast as the Least-Square Solver.\n",
    "- The running time for both algorithms is quite fast (ODG: )\n",
    "\n",
    "### Drawbacks\n",
    "- Compared to other results with the Jester dataset, we should have obtained a RMSE score ranked between 7 and 4. To prove this assumption, we noticed that, given 2 random matrices $W$ and $H$, the RMSE was lower than for our 2 NMF algorithms.\n",
    "- This is due to overfitting. The matrix factorization can overfit for users with few (no more than $K$) ratings: assuming that the feature vectors $H$ of the items  rated by the user are linearly independent and $H$ does not change, there exists a user feature vector $W_u$ with zero training error. Thus, there is a potential for overfitting, if $\\eta$ and the extent of the change in $H$ are both small.\n",
    "- A common way to avoid overfitting is to apply regularization by fixing the dictionary $W$ and by penalizing the square of the Euclidean norm of the weights, i.e. the features matrix of items $H$. NMF induces sparsity a but, but it is not enough to avoid overfitting.\n",
    "- Another drawback is that these 2 algorithms cannot be used for real data, that is to say, they are not scalable. Indeed, the Jester dataset is very small compared to MovieLens for example. In addition, we have only taken a small part of this dataset.\n",
    "- Finally, the 2 algorithms don't deal with missing values: we can only measure the RMSE if the rating matrix is dense. However, this never appears with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# IV / Link with the paper K-Dimensional Coding Schemes in Hilbert Spaces\n",
    "\n",
    "n order to link the NMF to the paper, let's take a look at the reconstruction error defined in the paper:\n",
    "$$ f_T(x) = \\min_{y \\in Y}{\\| x - T y\\|^2} $$\n",
    "and\n",
    "$$ \\hat{y} = \\text{arg } \\min_{y \\in Y}{\\| x - T y\\|^2} $$\n",
    "\n",
    "To approximate $\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x)$, we use the empirical risk defined as follows: \n",
    "$$\\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i)$$\n",
    "which corresponds to the MSE in our case, with $ x_i$ a user i of the rating matrix $R$\n",
    "\n",
    "The general theorem of the autors give us a bound on the \"regret\" $ \\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i)$ with at least probability $1 - \\delta$ as follows:\n",
    "\n",
    "$$\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i) \\leq \\frac{K}{\\sqrt{m}}\\left(14 \\| \\mathcal{T} \\|_{Y} + \\frac{b}{2} \\sqrt{\\ln{16 m \\| \\mathcal{T}}\\|_Y^2} \\right) + b \\sqrt{\\frac{\\ln{1/\\delta}}{2 m}}$$\n",
    "with the following notations:\n",
    "$$\\mathcal{T} = \\{T: T \\in \\mathcal{L}(\\mathbb{R}^K, H)$$\n",
    "\n",
    "$$ \\| \\mathcal{T} \\|_Y = \\sup_{T \\in \\mathcal{T}}{\\| T \\|_Y} = \\sup_{T \\in \\mathcal{T}}{\\sup_{y \\in Y}{\\| T y\\|}} $$\n",
    "\n",
    "In the case of NMF, we have the following constraints :\n",
    " $$\\|T e_k \\| = 1, \\langle T e_k , T e_l\\rangle \\geq 0, 1 \\leq k,l \\leq K$$\n",
    "\n",
    "Here $T$ refers to the matrix $W$ - i.e. the dictionary $D$ we defined in the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\{e_1...e_K\\}$ be the canonical base of $\\mathbb{R}^K$.\n",
    "To apply this bound to the NMF framework we need to be able to fix $b$ and $||\\mathcal{T}||_Y$\n",
    "In the particular case of NMF, all the columns of T are positive and Y is positive and restricted to the $l_1$-ball of $\\mathbb{R}^K$ ie $\\forall i$ we have $y_i<1$. Thus we have :\n",
    "\n",
    "$$\n",
    "||Ty|| = ||\\sum_{k=1}^{K} y_k T e_k ||\n",
    "$$\n",
    "$$\n",
    "\\leq \\sum_{k=1}^{K} |y_k| ||T e_k|| \\text{ with triangle inequality}\n",
    "$$\n",
    "$$\n",
    "= \\sum_{k=1}^{K} y_k ||T e_k|| \\text{ because y postive}\n",
    "$$\n",
    "$$\n",
    "\\leq \\sum_{k=1}^{K} ||T e_k|| \n",
    "$$\n",
    "$$\n",
    "\\leq (\\sum_{k=1}^{K} ||T e_k||^2)^{1/2}\n",
    "$$\n",
    "$$\n",
    "= K^{1/2} \\text{ in the case of NMF, because here $||Te_k|| = 1$ thanks to a re-normalization without loss of generality thanks to Lemma 2 demonstrated by the autors.}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "||\\mathcal{T}||_Y = \\sup_{T \\in \\mathcal{T}} \\sup_y ||Ty|| \\leq K^{1/2}\n",
    "$$\n",
    "\n",
    "So, for NMF, we can use the bound of theorem 2 with $||\\mathcal{T}||_Y = K^{1/2}$ and $b=0$ which give the form of the NMF bound proposed by the autors :\n",
    "\n",
    " $$\\mathbb{E}_{x \\text{ ~ } \\mu} f_T(x) - \\frac{1}{m} \\sum \\limits_{i=1}^m f_T(x_i) \\leq \\frac{K}{\\sqrt{m}}\\left(14 \\sqrt{K} + \\sqrt{\\ln{16 m K}} \\right) + \\sqrt{\\frac{\\ln{1/\\delta}}{2 m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# V / Going further than NMF : Online Structured Dictionary Learning (OSDL)\n",
    "\n",
    "Here, the paper we studied is the following : https://www.cs.cmu.edu/~bapoczos/articles/szabo12collaborative.pdf. However, this paper doesn't provide updates rules and practical details about the algorithm used. We studied, then, their other paper : http://www.gatsby.ucl.ac.uk/~szabo/publications/szabo11online_with_supplementary_material.pdf which provides the pseudo-code (see supplementary part) of the algorithm they proposed.\n",
    "\n",
    "## Main differences with Seung & Lee algorithm\n",
    "\n",
    "1. This is an online algorithm (to be more precise, this algorithm is presented as a generalization of Mairal, Bach, Ponce & Sapiro's Online Dictionary Learning : http://www.di.ens.fr/willow/pdfs/icml09.pdf).\n",
    "2. It takes into account overlapping group structures which could be present in the observed data, thanks to a special penalty term.\n",
    "3. Applies non-convex sparsity inducing regularization.\n",
    "4. Can deal with missing information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overwiew of the problem\n",
    "\n",
    "The algorithm is based, like the NMF, on a alternated minimization scheme.\n",
    "\n",
    "### Minimization on alpha\n",
    "\n",
    "Here, because we are solving an online matrix factorization problem, we don't see the whole dataset $X$. That's why, for a fixed D we have the following problem to solve :\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\frac{1}{2} ||x - D\\alpha||^2_2 + \\kappa\\Omega(\\alpha)\n",
    "$$\n",
    "$$\n",
    "x \\in \\mathbb{R}^{d_x}\n",
    "$$\n",
    "$$\n",
    "\\alpha \\in \\mathbb{R}^{d_\\alpha}\n",
    "$$\n",
    "$$\n",
    "D \\in \\mathbb{R}^{d_x,d_\\alpha}\n",
    "$$\n",
    "$$\n",
    "\\kappa > 0\n",
    "$$\n",
    "\n",
    "Here the penalty is more complex than just a classical $l_1$ penalty : \n",
    "\n",
    "$$\n",
    "\\Omega(\\alpha) = ||(||d^g \\odot \\alpha||_2)_{g \\in G}||_\\eta\n",
    "$$\n",
    "\n",
    "- $d^g$ is a vector containing 1 on indices contained in group $g$ and 0 otherwise.\n",
    "- $g$ is the index of a group of indices belonging to $G$, a set of groups (called structure). If $G$ is a partition of $\\{1...p\\}$ and $\\eta$ is fixed to 1 then it is very easy to see that this penalty is simply the group Lasso. If $G$ is just the singletons $\\{i\\}$ with $i \\in \\{1..p\\}$ and $\\eta$ fixed to 1, the penalty is just the classical Lasso penalty.\n",
    "\n",
    "### Minimization on D\n",
    "#### Basic case\n",
    "Notation :\n",
    "$$\n",
    "l(x_i,D) = \\min_{\\alpha}[ \\frac{1}{2} ||x_i - D\\alpha||^2_2 + \\kappa\\Omega(\\alpha)]\n",
    "$$\n",
    "Here, when the $\\alpha$ minimizing the quantity above is known, we want to minimize the following problem :\n",
    "$$\n",
    "\\min_{D} \\frac{1}{K} \\sum_{i=1}^{T} w_i l(x_i,D)\n",
    "$$\n",
    "$$\n",
    "w_i = (\\frac{i}{T})^\\rho\n",
    "$$\n",
    "- Here it is important to notice that they introduce a weighting term $w_i$ which is decreasing when T increases. That is to say, an old example will have a lot less weight in the quantity to minimize than a recently seen example. Thus, the algorithm is fully online : if we keep it running provinding it an endless flow of data, it will continuously adapt itself to the new data points observed.\n",
    "- K is just a normalization factor to make the sum of the weights equal to one.\n",
    "- $\\rho$ is a \"forgetting factor\", quantifying how fast the algorithm will forget the past examples when fitting D. The fact is that when you put $\\rho$ equal to 0 you get the minimization problem already covered in Mairal's paper : the function is then, just an average of the individual losses of the examples seen.\n",
    "\n",
    "#### Dealing with missing data points\n",
    "When there are missing data points in the training set (for example, in the Jester, missing jokes ratings), the autor proposed a more general reformulation of the problem :\n",
    "\n",
    "$$\n",
    "l(x_i,D) = \\min_{\\alpha}[ \\frac{1}{2} ||\\Delta_i(x_i - D\\alpha)||^2_2 + \\kappa\\Omega(\\alpha)]\n",
    "$$\n",
    "With $\\Delta_i$ a diagonal matrix of the following form :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\Delta_i)_{q,q} =\n",
    "  \\left\\{\n",
    "    \\begin{split}\n",
    "     1 \\text{ if   the qth item for the user i is observable}\\\\ \n",
    "     0 \\text{ otherwise} \\\\\n",
    "    \\end{split}\n",
    "  \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Then he can computes the form of the updates rules just taking a gradient and solving the linear system :\n",
    "$$\n",
    "D^* = argmin\n",
    "<=>\n",
    "\\nabla_D[ \\frac{1}{K} \\sum_{i=1}^{T}\\frac{w_i}{2} ||\\Delta_i(x_i - D\\alpha)||^2_2 + \\kappa\\Omega(\\alpha) ] = 0\n",
    "<=>\n",
    "\\nabla_D[ \\sum_{i=1}^{T}w_i ||\\Delta_i(x_i - D\\alpha)||^2_2 ] = 0 \\text{ because here $\\alpha$ is fixed}\n",
    "$$\n",
    "\n",
    "The resolution of this equation and some tricks fully described in the paper allow the autors to get the update rule for D.\n",
    "In particular, to do so, they are obliged (as Mairal in his own online learning algo) to introduce some intermediary matrices B,C, and E to compute the update of D (see the pseudo-code in next section).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the OSDL algorithm\n",
    "Before starting to code the OSDL algorithm we need 2 functions :\n",
    "- a function to generate the structure used as an input argument for the OSDL algorithm\n",
    "- a function to compute the $\\Delta_i$ matrices for each new line of data we see\n",
    "Here are the implementations of these two functions :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_structure(d_alpha, struct, nb_group=3):\n",
    "    if struct == 'lasso':\n",
    "        d_G = []\n",
    "        for i in range(d_alpha):\n",
    "            G = np.zeros(d_alpha)\n",
    "            G[i] = 1\n",
    "            d_G.append(G)\n",
    "            \n",
    "    if struct == 'group_lasso':\n",
    "        # fonction rand pour donner le nombre d'indices dans chaque partition de {1...d_alpha}\n",
    "        # Exemple: n paritions\n",
    "        partition = sorted(np.random.choice(d_alpha, nb_group-1, replace = False))\n",
    "\n",
    "        ## shuffle les indices\n",
    "        shuffled_indices = np.random.choice(d_alpha, d_alpha, replace = False)\n",
    "        \n",
    "        ## Création des groupes\n",
    "        x = []\n",
    "        for i, item in enumerate(partition):\n",
    "            if i == 0:\n",
    "                x.append(shuffled_indices[:item])\n",
    "            elif i == len(partition) - 1:\n",
    "                x.append(shuffled_indices[partition[i-1]:item])\n",
    "                x.append(shuffled_indices[item:])\n",
    "            else:\n",
    "                x.append(shuffled_indices[partition[i-1]: item])\n",
    "        d_G = []\n",
    "        for i, item in enumerate(x):\n",
    "            G = np.zeros(d_alpha)\n",
    "            G[item] = 1\n",
    "            d_G.append(G)\n",
    "            \n",
    "    return d_G\n",
    "\n",
    "def delta(r,X_batch):\n",
    "    X_r = X_batch[:,r]\n",
    "    delta = np.eye((d_x))\n",
    "    non_observable = X_r==99\n",
    "    delta[non_observable,non_observable]=0\n",
    "    return np.array(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, the pseudo-code for the OSDL is the following.\n",
    "<img src='Pseudo-code-main.png'>\n",
    "\n",
    "Our implementation is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OSDL(X,D0,T,R,Group_struct,rho,kapa,eta,epsilon = 0.00001,T_alpha = 5,T_D = 5):\n",
    "    d_x, N = X.shape\n",
    "    C = np.zeros((d_x,d_x,d_alpha))\n",
    "    B = np.zeros((d_x,d_alpha))\n",
    "    E = np.zeros((d_x,d_alpha))\n",
    "    alpha = np.random.rand(d_alpha,R)\n",
    "    D = np.random.rand(d_x,d_alpha)\n",
    "    \n",
    "    for t in range(T):\n",
    "        print \"iteration N° : \",t\n",
    "        #Draw samples for mini-batches\n",
    "        X_batch = X[:,t*R:(t+1)*R] #Dans notre cas on coupe artificiellement le dataset en batchs\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------        \n",
    "        #Compute alphas_r representations\n",
    "        for r in range(R):\n",
    "            x_temp = list(X_batch[:,r])\n",
    "            ## Save indexes of observed values\n",
    "            indices = [i for i, item in enumerate(x_temp) if item != 99]\n",
    "            #remove missing values of first row of R_train\n",
    "            x_O = [a for a in x_temp if a != 99]\n",
    "            # save the observed rows of D\n",
    "            D_O = D[np.array(indices),:].copy() \n",
    "            #Compute the r column of alpha\n",
    "            alpha[:,r] = min_alpha(x_O, D_O, alpha[:,r], eta, d_alpha, T_alpha, epsilon, d_G)\n",
    "        \n",
    "        \n",
    "        #----------------------------------------------------------------------------------------    \n",
    "        #Update the statistics\n",
    "        gamma=(1-1/float(t+1))**rho\n",
    "        \n",
    "        #Update B\n",
    "        sum_b = np.zeros((d_x,d_alpha))\n",
    "       \n",
    "        for r in range(R):\n",
    "            sum_b += np.dot(np.expand_dims(np.dot(delta(r,X_batch),X_batch[:,r]),axis=1),np.expand_dims(alpha[:,r],axis=1).T)\n",
    "        B = gamma*B + 1/float(R)*sum_b\n",
    "        \n",
    "        #Update all the C_j matrices and e_j vectors\n",
    "        sum_c = np.zeros((d_x,d_x))\n",
    "        \n",
    "        for j in range(d_alpha):\n",
    "            C_j = C[:,:,j]\n",
    "            E[:,j] = gamma*-E[:,j] #partial update on e_j\n",
    "            \n",
    "            for r in range(R):\n",
    "                sum_c += delta(r,X_batch)*alpha[j,r]**2\n",
    "            \n",
    "            C_j = gamma*C_j + 1/float(R)*sum_c\n",
    "            C[:,:,j] = C_j\n",
    "        #----------------------------------------------------------------------------------------\n",
    "        #Update dictionnary \n",
    "        D = dictionary(C,B,E,X_batch,T_D, alpha,D,d_x,d_alpha,R)\n",
    "        #----------------------------------------------------------------------------------------\n",
    "        #Finish update on E\n",
    "        sum_e = np.zeros(d_x)\n",
    "        for j in range(d_alpha):\n",
    "            e_j = E[:,j]\n",
    "            for r in range(R):\n",
    "                sum_e += np.dot(np.dot(delta(r,X_batch),D),alpha[:,r])*alpha[j,r]\n",
    "                \n",
    "            e_j = e_j + 1/R*sum_e\n",
    "            E[:,j] = e_j #final update\n",
    "        \n",
    "    \n",
    "    return D    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-code for the update of the alphas is given here :\n",
    "<img src='Pseudo-code-representation.png'>\n",
    "\n",
    "Our implementation is the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_alpha(x_O, D_O, alpha, eta, d_alpha, T_alpha, epsilon, d_G):\n",
    "    ## Minimisation on alpha\n",
    "    for t in range(T_alpha):\n",
    "\n",
    "        ## Compute z\n",
    "        norm_G_alpha = []\n",
    "\n",
    "        for j in d_G:\n",
    "            norm_G_alpha.append(np.linalg.norm(j * alpha, ord =2))\n",
    "        coef = (np.linalg.norm(norm_G_alpha, ord = eta) ** (eta - 1))\n",
    "\n",
    "        norm_G_alpha = np.array(norm_G_alpha)\n",
    "        norm_G_alpha = np.power(norm_G_alpha, 2 - eta)\n",
    "        z = norm_G_alpha * coef\n",
    "        for i, item in enumerate(z):\n",
    "            z[i] = max(item, epsilon)\n",
    "\n",
    "        \n",
    "        ## Compute alpha\n",
    "        ksi = []\n",
    "        for j in range(d_alpha):\n",
    "            coef_ksi = 0.0\n",
    "            for i, item in enumerate(z):\n",
    "                coef_ksi += (d_G[i][j])**2 / item \n",
    "            ksi.append(coef_ksi)\n",
    "        \n",
    "        ### Pour résoudre le pb de progammation quadratique, on utilise cvxopt\n",
    "        # P = kappa * diag(ksi) + D_O.T.dot(D_O)\n",
    "        # Q = - D_O.T.dot(x_O)\n",
    "        P = kappa * np.diag(ksi) + D_O.T.dot(D_O)\n",
    "        P = cvxopt.matrix(P)\n",
    "\n",
    "        q = - D_O.T.dot(x_O)\n",
    "        q = cvxopt.matrix(q)\n",
    "\n",
    "        sol = cvxopt.solvers.qp(P,q)\n",
    "        alpha = np.ravel(sol['x'])\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-code proposed for the update of the dictionary is the following :\n",
    "<img src='Pseudo-code-dictionary.png'>\n",
    "\n",
    "We propose here the following implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary(C,B,E,X_batch,T_D, alpha,D,d_x,d_alpha,R):\n",
    "       \n",
    "    #NOTE ici alpha est la concatenation des vecteurs alphas updatés précédement, pour chaque ligne du batch (R lignes). \n",
    "    #C'est une matric de taille (d_alpha,R)\n",
    "    \n",
    "    for t in range(T_D):\n",
    "        for j in range(d_alpha):\n",
    "            #compute e_j\n",
    "            sum_r=0\n",
    "            for r in range(int(R)) :  \n",
    "                sum_r += np.dot(np.dot(delta(r,X_batch),D),alpha[:,r])*alpha[j,r]\n",
    "            e_j_temp = E[:,j] + 1/R * sum_r\n",
    "            #compute u_j by solving linear system\n",
    "            C_j = C[:,:,j]\n",
    "            u = np.linalg.solve(C_j, B[:,j]-e_j_temp+np.dot(C_j,D[:,j]))\n",
    "            #u = np.linalg.lstsq(C_j, B[:,j]-e_j_temp+np.dot(C_j,D[:,j]))[0]\n",
    "            #contraints u_j to keep only positive coefficients\n",
    "            u[u<0]=0\n",
    "            #compute d_j by projection of u on the good space\n",
    "            D[:,j] = u / max(np.linalg.norm(u),1)\n",
    "            \n",
    "    return D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "### Similarities with Mairal's paper\n",
    "First, as far as the algorithm is online, we can notice some __similarities__ with Mairal's algorithm : \n",
    "- This is a alternated minimization scheme, on alpha and on D.\n",
    "- Alpha is updated with a fixed D using a quadratic programing solver (for ex the one implemented in cvxopt). Mairal proposed, on the other hand, a LARS solver (for ex the one implemented in sklearn)\n",
    "- The dictionary is updated using intermediary matrices : A,B in Mairal, E,B,C,delta in this paper.\n",
    "\n",
    "### Differences with Mairal's paper\n",
    "- We use batches of data. That is to say, instead of doing the online procedure line of observation by line of observation, we take multiple lines in the same time, we concantenate them and, then, start updating alpha and D.\n",
    "- Because the penalty is a lot more complex than Mairal's one, the update of alpha is different and needs extra-steps before calling the solver (i.e computing the \"argmin step\"). \n",
    "- Mairal paper doesn't talk about incomplete data. In fact, here, most of our entries are incomplete. That's why, in the updates formulas, there is a $\\Delta_i$ matrix ivolved while this factor is absent of Mairal work. \n",
    "- Finally, because the function to optimize in D with a regret factor (not implemented in Mairal's) and because of the previous point, the updates rules are more complex. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning D and predicting ratings\n",
    "\n",
    "Here, because the algorithm is online we cannot use a prediction of the form :\n",
    "$$\n",
    "\\hat{X} = DA\n",
    "$$\n",
    "because the learning algorithm is only able to return a learnt dictionary $D$. The $\\alpha$s are scratched at each iteration. Thus, we used the following technique :\n",
    "- Learn D on a train dataset with the OSDL algorithm\n",
    "- Then, iteratively, make a prediction for a new data point belonging to the test data set letting D unmodified and modifying the $\\alpha$ at each step, like in the training phase. At each step the prediction is then :\n",
    "$$\n",
    "\\hat{x} = D\\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train = ratings[:10000]\n",
    "R_val = ratings[10000:20000]\n",
    "R_test = ratings[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_alpha = 100\n",
    "d_x = 100\n",
    "alpha = np.random.randn(d_alpha)\n",
    "#Number of iterations in the alpha update\n",
    "T_alpha = 5\n",
    "#Number of iterations in the D update\n",
    "T_D = 5\n",
    "#Number of iterations in the OSDL learning (corresponding to the number of batches)\n",
    "T = 150\n",
    "#Size of batches\n",
    "R = 8\n",
    "#Regularization parameters\n",
    "epsilon = 10**(-5)\n",
    "rho = 32\n",
    "eta = 0.5\n",
    "kappa = 1. / (2**10)\n",
    "#Structure\n",
    "d_G = group_structure(d_alpha, struct='lasso', nb_group=10)\n",
    "#Initialization dictionary\n",
    "D0 = np.random.rand(d_x,d_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration N° :  0\n",
      "iteration N° :  1\n",
      "iteration N° :  2\n",
      "iteration N° :  3\n",
      "iteration N° :  4\n",
      "iteration N° :  5\n",
      "iteration N° :  6\n",
      "iteration N° :  7\n",
      "iteration N° :  8\n",
      "iteration N° :  9\n",
      "iteration N° :  10\n",
      "iteration N° :  11\n",
      "iteration N° :  12\n",
      "iteration N° :  13\n",
      "iteration N° :  14\n",
      "iteration N° :  15\n",
      "iteration N° :  16\n",
      "iteration N° :  17\n",
      "iteration N° :  18\n",
      "iteration N° :  19\n",
      "iteration N° :  20\n",
      "iteration N° :  21\n",
      "iteration N° :  22\n",
      "iteration N° :  23\n",
      "iteration N° :  24\n",
      "iteration N° :  25\n",
      "iteration N° :  26\n",
      "iteration N° :  27\n",
      "iteration N° :  28\n",
      "iteration N° :  29\n",
      "iteration N° :  30\n",
      "iteration N° :  31\n",
      "iteration N° :  32\n",
      "iteration N° :  33\n",
      "iteration N° :  34\n",
      "iteration N° :  35\n",
      "iteration N° :  36\n",
      "iteration N° :  37\n",
      "iteration N° :  38\n",
      "iteration N° :  39\n",
      "iteration N° :  40\n",
      "iteration N° :  41\n",
      "iteration N° :  42\n",
      "iteration N° :  43\n",
      "iteration N° :  44\n",
      "iteration N° :  45\n",
      "iteration N° :  46\n",
      "iteration N° :  47\n",
      "iteration N° :  48\n",
      "iteration N° :  49\n",
      "iteration N° :  50\n",
      "iteration N° :  51\n",
      "iteration N° :  52\n",
      "iteration N° :  53\n",
      "iteration N° :  54\n",
      "iteration N° :  55\n",
      "iteration N° :  56\n",
      "iteration N° :  57\n",
      "iteration N° :  58\n",
      "iteration N° :  59\n",
      "iteration N° :  60\n",
      "iteration N° :  61\n",
      "iteration N° :  62\n",
      "iteration N° :  63\n",
      "iteration N° :  64\n",
      "iteration N° :  65\n",
      "iteration N° :  66\n",
      "iteration N° :  67\n",
      "iteration N° :  68\n",
      "iteration N° :  69\n",
      "iteration N° :  70\n",
      "iteration N° :  71\n",
      "iteration N° :  72\n",
      "iteration N° :  73\n",
      "iteration N° :  74\n",
      "iteration N° :  75\n",
      "iteration N° :  76\n",
      "iteration N° :  77\n",
      "iteration N° :  78\n",
      "iteration N° :  79\n",
      "iteration N° :  80\n",
      "iteration N° :  81\n",
      "iteration N° :  82\n",
      "iteration N° :  83\n",
      "iteration N° :  84\n",
      "iteration N° :  85\n",
      "iteration N° :  86\n",
      "iteration N° :  87\n",
      "iteration N° :  88\n",
      "iteration N° :  89\n",
      "iteration N° :  90\n",
      "iteration N° :  91\n",
      "iteration N° :  92\n",
      "iteration N° :  93\n",
      "iteration N° :  94\n",
      "iteration N° :  95\n",
      "iteration N° :  96\n",
      "iteration N° :  97\n",
      "iteration N° :  98\n",
      "iteration N° :  99\n",
      "iteration N° :  100\n",
      "iteration N° :  101\n",
      "iteration N° :  102\n",
      "iteration N° :  103\n",
      "iteration N° :  104\n",
      "iteration N° :  105\n",
      "iteration N° :  106\n",
      "iteration N° :  107\n",
      "iteration N° :  108\n",
      "iteration N° :  109\n",
      "iteration N° :  110\n",
      "iteration N° :  111\n",
      "iteration N° :  112\n",
      "iteration N° :  113\n",
      "iteration N° :  114\n",
      "iteration N° :  115\n",
      "iteration N° :  116\n",
      "iteration N° :  117\n",
      "iteration N° :  118\n",
      "iteration N° :  119\n",
      "iteration N° :  120\n",
      "iteration N° :  121\n",
      "iteration N° :  122\n",
      "iteration N° :  123\n",
      "iteration N° :  124\n",
      "iteration N° :  125\n",
      "iteration N° :  126\n",
      "iteration N° :  127\n",
      "iteration N° :  128\n",
      "iteration N° :  129\n",
      "iteration N° :  130\n",
      "iteration N° :  131\n",
      "iteration N° :  132\n",
      "iteration N° :  133\n",
      "iteration N° :  134\n",
      "iteration N° :  135\n",
      "iteration N° :  136\n",
      "iteration N° :  137\n",
      "iteration N° :  138\n",
      "iteration N° :  139\n",
      "iteration N° :  140\n",
      "iteration N° :  141\n",
      "iteration N° :  142\n",
      "iteration N° :  143\n",
      "iteration N° :  144\n",
      "iteration N° :  145\n",
      "iteration N° :  146\n",
      "iteration N° :  147\n",
      "iteration N° :  148\n",
      "iteration N° :  149\n"
     ]
    }
   ],
   "source": [
    "X = R_train.T\n",
    "D = OSDL(X,D0,T,R,d_G,rho,kappa,eta,epsilon,T_alpha,T_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Take 1st row of R_train\n",
    "x = list(R_train[0].copy())\n",
    "\n",
    "## Save indexes of observed values\n",
    "indices = [i for i, item in enumerate(x) if item != 99]\n",
    "\n",
    "#remove missing values of first row of R_train\n",
    "x_O = [a for a in x if a != 99]\n",
    "\n",
    "# save the observed rows of D\n",
    "D_O = D[np.array(indices),:].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testing_model(D, alpha, nb_items_to_remove, R_test, model = 'NMF', missing_value = 99):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    ## Define testing set\n",
    "    # On définit le testing set, on fait la prédiction et on calcule l'erreur dans 1 même boucle\n",
    "    \n",
    "    N = R_test.shape[0]\n",
    "    MSE = 0.0\n",
    "    #if model == 'NMF':\n",
    "     #   XXXXXXXX\n",
    "    if model == 'OSDL':\n",
    "        for j, jtem in enumerate(R_test):\n",
    "            ## Take 1st row of R_train\n",
    "            x = jtem.copy()\n",
    "\n",
    "            ## Save indexes of observed values\n",
    "            indices = [i for i, item in enumerate(x) if item != missing_value]\n",
    "            indices = np.array(indices)\n",
    "\n",
    "            # save the observed rows of D\n",
    "            # we will use for prediction of x_O_pred and compare it to x_O\n",
    "            D_O_test = D[np.array(indices),:].copy() \n",
    "\n",
    "            #remove missing values of first row of R_train\n",
    "            x_O_test = x[indices]\n",
    "\n",
    "            #remove nb_items_to_remove values of x_O_test\n",
    "            index_pred = sorted(np.random.choice(len(x_O_test), len(x_O_test) - nb_items_to_remove, replace = False))\n",
    "            x_O_pred = x_O_test[index_pred].copy()\n",
    "            D_O_pred = D[index_pred,:].copy()\n",
    "\n",
    "            #prediction\n",
    "            alpha_test = min_alpha(x_O_pred, D_O_pred, alpha, eta, d_alpha, T_alpha, epsilon, d_G)\n",
    "\n",
    "            MSE += mean_squared_error(x_O_test, D_O_test.dot(alpha_test))\n",
    "\n",
    "        MSE = MSE / N\n",
    "        RMSE = np.sqrt(MSE)\n",
    "    return 'MSE: ',MSE, ' RMSE: ', RMSE\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MSE: ', 23.990990881590296, ' RMSE: ', 4.8980599099633615)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_model(D, alpha, 10, R_test, model = 'OSDL', missing_value = 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a RMSE which is in the order of what is exepected on this dataset (<5) : see Kaggle competition on this particular dataset for instance (https://inclass.kaggle.com/c/uci77B/leaderboard). However, we are still far from the results achieved by the autors of the paper. Multiple reasons could explain that :\n",
    "- They use a better tuned group-structure whereas our dictionnary is based on a simple Lasso\n",
    "- They tuned the set of hyperparameters better (with cross-validation)\n",
    "- Something is going wrong in our implementation of the OSDL learning process (but because of the complexity of the code, this is really hard to check). However, because the testing phase seems to work pretty well, the problem may come from the dictionary update phase...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define similarities\n",
    "\n",
    "# Normalized scalar product based similarity\n",
    "def item_similarity_1(q_i, q_j, alpha):\n",
    "    a = (q_i ).T.dot(q_j)\n",
    "    b = q_i.T.dot(q_i)\n",
    "    c = q_j.T.dot(q_j)\n",
    "    \n",
    "    s_ij = (max(0, a) / np.sqrt(b * c))** alpha\n",
    "    \n",
    "    return s_ij\n",
    "\n",
    "# Normalized Euclidian distance based similarity\n",
    "def item_similarity_2(q_i, q_j, alpha):\n",
    "    a = (q_i - q_j).T.dot(q_i - q_j)\n",
    "    b = q_i.T.dot(q_i)\n",
    "    c = q_j.T.dot(q_j)\n",
    "    \n",
    "    s_ij = (a / np.sqrt(b * c))**(- alpha)\n",
    "    \n",
    "    return s_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion on OSDL\n",
    "This algorithm has multiple advantages and drawbacks, here are the main :\n",
    "\n",
    "### Advantages\n",
    "- The OSDL algorithm is able to penalize group structures, a lot more complex than simple lasso. This is particularly interesting in image processing where the images could have group-of-pixels structures. Moreover, the wavelet transform has a tree structure which could also be used using OSDL.\n",
    "- The OSDL algorithm is batch (increase of speed) and online. Thus it can deals with continuous incoming flows of data.\n",
    "- It is flexible in time thanks to the regret term. The paper proposed by Mairal didn't have this particularity, resulting, after too much iterations, in an avareging of the dictionary (this is bad beacuse if the atoms of the dictionary are too similar, its expressivity is low)\n",
    "- It is able to treat incomplete data, but, then, the reconstruction error could be really bad.\n",
    "\n",
    "### Drawbacks\n",
    "- The pseudo-code proposed by the autors is excessively complex, especially if you compare it to Seug & Lee or to Mairal papers.\n",
    "- This method is not scalable ! It requires, for the update of D, to store in memory $d_\\alpha$ matrices $C_j$ of size $(d_x,d_x)$. This method is particularly not implementable if $d_x$ is high, that is to say if we have a lot of ratings per user. Maybe it explains why the autors chose the Jester dataset ! (which is of size $d_x$ particularly small !)\n",
    "- Of course, if the dataset was larger, we would also face a slow down when computing the alpha and D updates with quadratic solvers...\n",
    "- The pseudo-code proposed is not optimized. If you look carefully, we compute some updates in the main loop which are then recomputed in the dictionary update loop. This could have been treated simply, to make things more understandable, we wanted to stick to the paper formulations.\n",
    "- We don't see how, on the specific problem of collaborative filtering on the Jester dataset, a group structure can be exhibited. We have no information about similarities between items. Thus, it seems strange to prior iteractions/structure between items... Maybe a \"bruteforce\" approach could have improve the results : simulate randomly a structure, train D, assess the performances, restart a high number of time and keep the best performing random structure.\n",
    "- This algorithm introduce a lot of hyperparameters ! This could make it more flexible to different datasets, but in the mean time it increases a lot the complexity of the training phase : the only way to fix the hyperparameters is to do extensive grid searching. This operation, of course, is computationally heavy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
